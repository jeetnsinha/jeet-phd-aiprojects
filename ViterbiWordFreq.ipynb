{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpUDJ+ANgYISb3KaQ57cVr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeetnsinha/jeet-phd-aiprojects/blob/main/ViterbiWordFreq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO8AhaZga1PU",
        "outputId": "d5519f64-4f08-45f9-e00a-5478dd56f231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'longest', 'list', 'of', 'the', 'longest', 'stuff', 'at', 'the', 'longest', 'domain', 'name', 'at', 'long', 'last', 'com']\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# Function to compute the probability of a word given the total word count\n",
        "def calculate_word_probability(word, total_word_count, word_statistics):\n",
        "    if word in word_statistics:\n",
        "        return -math.log(word_statistics[word] / total_word_count)\n",
        "    else:\n",
        "        return float('inf')  # Return high cost for unknown words\n",
        "\n",
        "# Viterbi algorithm for segmenting text\n",
        "def segment_text_viterbi(input_text, word_statistics):\n",
        "    text_length = len(input_text)\n",
        "    total_word_count = sum(word_statistics.values())  # Total word count for probability calculation\n",
        "\n",
        "    optimal_segmentation = [None] * (text_length + 1)\n",
        "    optimal_score = [float('inf')] * (text_length + 1)\n",
        "\n",
        "    # Base case - zero cost for empty prefix\n",
        "    optimal_score[0] = 0\n",
        "\n",
        "    for end in range(1, text_length + 1):\n",
        "        for start in range(end):\n",
        "            segment = input_text[start:end]\n",
        "            score = optimal_score[start] + calculate_word_probability(segment, total_word_count, word_statistics)\n",
        "            if score < optimal_score[end]:\n",
        "                optimal_score[end] = score\n",
        "                optimal_segmentation[end] = (start, segment)\n",
        "\n",
        "    # Reconstruct the optimal word segmentation\n",
        "    index = text_length\n",
        "    final_segments = []\n",
        "    while index > 0:\n",
        "        start, segment = optimal_segmentation[index]\n",
        "        final_segments.append(segment)\n",
        "        index = start\n",
        "\n",
        "    return final_segments[::-1]\n",
        "\n",
        "# The URL to be segmented\n",
        "input_url = \"thelongestlistofthelongeststuffatthelongestdomainnameatlonglastcom\"\n",
        "\n",
        "# Unigram word frequencies\n",
        "word_statistics = {\n",
        "    \"the\": 1000, \"longest\": 500, \"list\": 500, \"of\": 1000, \"stuff\": 100,\n",
        "    \"at\": 1000, \"domain\": 100, \"name\": 500, \"long\": 200, \"last\": 200,\n",
        "    \"com\": 1000\n",
        "}\n",
        "\n",
        "# Test the function with the URL\n",
        "segmented_result = segment_text_viterbi(input_url, word_statistics)\n",
        "\n",
        "# Print the segmented words\n",
        "print(segmented_result)\n"
      ]
    }
  ]
}